{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch autograd를 사용한 자동 미분",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOB3+eXi9OL2Ob4vUJ4zYPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaycold/pytorch_tutorial/blob/main/%2305.torch%20autograd%EB%A5%BC%20%EC%82%AC%EC%9A%A9%ED%95%9C%20%EC%9E%90%EB%8F%99%20%EB%AF%B8%EB%B6%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG6H9jN6oOhJ"
      },
      "source": [
        "## Torch.autograd를 사용한 자동 미분\n",
        "\n",
        "    : 신경망을 학습할 때 가장 자주 사용되는 알고리즘은 역전파\n",
        "    : 매개변수(모델가중치)는 주어진 매개변수에 대한 손실 함수의 변화도에 따라 조정\n",
        "    : torch.autograd라고 불리는 자동 미분 엔진이 내장\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB1oqZIgop1a"
      },
      "source": [
        "'''\n",
        "입력 : x // 매개변수 : w,b // \n",
        "단일 계층 신경망 구성\n",
        "'''\n",
        "\n",
        "import torch\n",
        "x = torch.ones(5) # INPUT TENSOR\n",
        "y = torch.zeros(3) # EXPECTED OUTPUT\n",
        "w = torch.randn(5,3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w) + b"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHqimMF4owHV",
        "outputId": "6f33cf99-9e85-42fb-a523-872a69e099cb"
      },
      "source": [
        "print(x)\n",
        "print(y)\n",
        "print('[w] \\n'  ,w)\n",
        "print(b)\n",
        "print(z)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "tensor([0., 0., 0.])\n",
            "[w] \n",
            " tensor([[ 0.0654,  0.7685,  1.3524],\n",
            "        [ 0.9088,  1.0460,  0.8048],\n",
            "        [ 0.2990, -0.2019, -0.9500],\n",
            "        [ 0.6548,  1.1785,  0.5599],\n",
            "        [-0.2507, -0.5031, -0.1404]], requires_grad=True)\n",
            "tensor([-1.1980, -0.0105,  0.2240], requires_grad=True)\n",
            "tensor([0.4794, 2.2774, 1.8507], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc7fJJtYpQPZ",
        "outputId": "407f0919-cb86-4395-9149-9c4ca192d1bd"
      },
      "source": [
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
        "loss"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7777, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay29vYp4w7gd"
      },
      "source": [
        "'''\n",
        "이 신경망에서 w와 b는 최적화를 해야하는 매개변수\n",
        "이러한 변수들에 대한 손실 함수의 변화도를 계산할 수 있어야\n",
        "이를 위해서 해당 텐서에 requires_grad 속성을 설정\n",
        "'''\n",
        "pass"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWSbae5L2Xgj"
      },
      "source": [
        "### 변화도 계산하기\n",
        "\n",
        "    : 신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대한 손실함수의 도함수를 계산하여야 함\n",
        "    : 즉 x와 y의 일부 고정값에서 w에 대한 loss 미분값, b에 대한 loss미분값이 필요함\n",
        "    : 이러한 도함수를 계산하기 위해 loss.backward()를 호출한 다음 w.grad와 b.grad에서 값을 가져옴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x10uOfvj27kH",
        "outputId": "384a6924-bca2-427f-d159-58824cac6c1f"
      },
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2059, 0.3023, 0.2881],\n",
            "        [0.2059, 0.3023, 0.2881],\n",
            "        [0.2059, 0.3023, 0.2881],\n",
            "        [0.2059, 0.3023, 0.2881],\n",
            "        [0.2059, 0.3023, 0.2881]])\n",
            "tensor([0.2059, 0.3023, 0.2881])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqvg0bX04z4I"
      },
      "source": [
        "##############\n",
        "# 연산 그래프의 잎 노드들 중 requires_grad 속성이 True로 설정된 노들의 grad속성만 구할 수있음\n",
        "# 다른 모든 노드에서는 변화도가 유효하지 않음\n",
        "# 성능상의 이유로, 주어진 그래프에서의 backward를 사용한 변화도 계산은 한번만 수행할 수 있음\n",
        "# 만약 동일한 그래프에서 여러번의 backward 호출이 필요하면, backward 호출 시에 retrain_graph = True를 전달해야함"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m2DkF535i0o"
      },
      "source": [
        "### 변화도 추적 멈추기\n",
        "\n",
        "    : 기본적으로, requires_grad = True인 모든 텐서들은 연산기록을 추적하고 변화도 계산을 지원함\n",
        "    : 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 순전파 연산이 필요한 경우에는 이러한 추적이나 지원이 필요없을 수 있음.\n",
        "    : torch.no_grad() 블록으로 둘러싸서 연산 추적을 멈출 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AKaE4Mp51I1",
        "outputId": "956bfa90-d7df-42d8-e7c5-400daf2ef972"
      },
      "source": [
        "z = torch.matmul(x, w) +b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad() :\n",
        "    z = torch.matmul(x,w) + b\n",
        "print(z.requires_grad)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10KrF9nx6Bp-"
      },
      "source": [
        "'''\n",
        "변화도 추적을 멈춰야 하는 이유\n",
        " - 신경망의 일부 매개변수를 고정된 매개변수(frozen parameter)로 표시\n",
        "  > 이는 사전학습된 신경망을 미세조정할때 매우 일반적인 시나리오\n",
        " - 변화도를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순전파 단계만 수행할 때 연산속도 향상\n",
        " '''\n",
        "pass"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGsGGZl-8uKz"
      },
      "source": [
        "### 연산 그래프에 대한 추가 정보\n",
        "\n",
        "    : 개념적으로 Autograd는 데이터(텐서)의 및 실행된 모든 연산들의 기록을 Function의 객체로 구성된 방향성 비순환 그래프에 저장\n",
        "    : 이 방향성 비순환 그래프의 잎은 입력 텐서, 뿌리는 결과 텐서\n",
        "\n",
        "    : 순전파 단계에서 autograd는 두 가지 작업 동시 수행\n",
        "     - 요청된 연산을 수행하여 결과 텐서 계산\n",
        "     - DAG에 연산의 변화도 기능을 유지\n",
        "\n",
        "    : 역전파 단계는 DAG 뿌리에서 .backward() 가 호출될 때 시작\n",
        "     - 각 .grad_fn으로부터 변화도를 계산\n",
        "     - 각 텐서의 .grad 속성에 계산 결과를 쌓고\n",
        "     - 연쇄 법칙을 사용하여, 모든 잎 텐서들까지 전파"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bya9jNr1gev"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}